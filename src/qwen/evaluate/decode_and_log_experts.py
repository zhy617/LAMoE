import os
import torch
import json
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm
from typing import Dict, List, cast
from transformers.models.qwen2_moe.modeling_qwen2_moe import Qwen2MoeForCausalLM

# å¯¼å…¥é¡¹ç›®é…ç½®
from ...config import (
    CURRENT_MODEL_PATH,
    MODEL_FULL_NAME,
    EVALUATE_DIR,
    TARGET_LAYERS,
    MODEL_FULL_DIR as BASE_MODEL_NAME,
    CURRENT_MODEL_PATH as BASE_MODEL_PATH,
)

# =========================================================
# 1. è¶…å‚æ•° (Hyperparameters)
# =========================================================
PROMPT = "Once upon a time, in a land far, far away,"
NUM_DECODE_TOKENS = 1024
BATCH_SIZE = 1 # å›ºå®šä¸º 1

# =========================================================
# 2. æ ¸å¿ƒåŠŸèƒ½ï¼šè§£ç å¹¶è®°å½•ä¸“å®¶é€‰æ‹©
# =========================================================
def decode_and_log_expert_activations(model, tokenizer, num_tokens_to_generate: int):
    """
    ä½¿ç”¨ bs=1 è¿›è¡Œè§£ç ï¼Œå¹¶é€ä¸ª token è®°å½•ä¸‹æ¯å±‚æ¿€æ´»çš„ä¸“å®¶ IDã€‚
    """
    model.eval()
    device = model.device

    # å‡†å¤‡è¾“å…¥
    inputs = tokenizer(PROMPT, return_tensors="pt").to(device)
    input_ids = inputs.input_ids

    # ç”¨äºŽå­˜å‚¨æ‰€æœ‰ token çš„ä¸“å®¶é€‰æ‹©æƒ…å†µ
    # ç»“æž„: {layer_idx: [token_1_experts, token_2_experts, ...]}
    # å…¶ä¸­ token_i_experts æ˜¯ä¸€ä¸ªåŒ…å« top-k ä¸ªä¸“å®¶ ID çš„åˆ—è¡¨
    all_expert_selections = {layer: [] for layer in TARGET_LAYERS}

    # èŽ·å– top_k å‚æ•°
    top_k = model.config.num_experts_per_tok

    expert_maps = {}
    for layer_idx in TARGET_LAYERS:
        try:
            expert_map = model.model.layers[layer_idx].mlp.expert_map
            expert_maps[layer_idx] = expert_map.to(device)
        except AttributeError:
            print(f"Warning: Could not find expert_map for layer {layer_idx}. Skipping.")


    print(f"ðŸš€ Starting generation for {num_tokens_to_generate} tokens with bs={BATCH_SIZE}...")
    print(f"Model will select top-{top_k} experts for each token.")
    with torch.no_grad():
        # ä½¿ç”¨ tqdm åˆ›å»ºè¿›åº¦æ¡
        pbar = tqdm(range(num_tokens_to_generate), desc="Generating tokens")
        for _ in pbar:
            # å…³é”®ï¼šè®¾ç½® output_router_logits=True æ¥èŽ·å–è·¯ç”±å™¨çš„è¾“å‡º
            outputs = model(
                input_ids,
                output_router_logits=True,
                use_cache=True, # åœ¨ç”Ÿæˆæ—¶å¿…é¡»ä½¿ç”¨ cache
            )

            # 1. èŽ·å–ä¸‹ä¸€ä¸ª token çš„ logits å¹¶ç”Ÿæˆæ–°çš„ token
            next_token_logits = outputs.logits[:, -1, :]
            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)

            # 2. è®°å½•å½“å‰ token çš„ä¸“å®¶é€‰æ‹©
            # outputs.router_logits æ˜¯ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«æ¨¡åž‹æ‰€æœ‰ MoE å±‚çš„ router_logits
            # æ¯ä¸ªå…ƒç´ çš„å½¢çŠ¶: (batch_size, sequence_length, num_experts)
            router_logits_per_layer = outputs.router_logits

            

            for layer_idx in TARGET_LAYERS:
                # æˆ‘ä»¬åªå…³å¿ƒæœ€æ–°ç”Ÿæˆçš„é‚£ä¸ª token çš„é€‰æ‹©æƒ…å†µ
                # router_logits çš„å½¢çŠ¶æ˜¯ (1, seq_len, num_experts)ï¼Œæˆ‘ä»¬å–æœ€åŽä¸€ä¸ª token
                # last_token_router_logits = router_logits_per_layer[layer_idx][0, -1, :]
                layer_router_logits = router_logits_per_layer[layer_idx]
                if layer_router_logits.dim() == 3:
                    last_token_router_logits = layer_router_logits[0, -1, :]
                else:
                    last_token_router_logits = layer_router_logits[-1, :]

                # è®¡ç®— top-k ä¸“å®¶
                _, selected_experts_original = torch.topk(last_token_router_logits, top_k)
                expert_map = expert_maps[layer_idx]
                selected_experts_final = expert_map[selected_experts_original]
                
                # è®°å½•ä¸“å®¶ ID
                all_expert_selections[layer_idx].append(selected_experts_final.cpu().tolist())

            # 3. æ›´æ–° input_ids ä»¥è¿›è¡Œä¸‹ä¸€æ¬¡è¿­ä»£
            input_ids = torch.cat([input_ids, next_token], dim=-1)

    print("âœ… Generation and logging complete.")
    return all_expert_selections

# ORIGINAL_MODEL_PATH = "/root/fsas/zhanghongyu/LAMoE/models/Qwen/Qwen1.5-MoE-A2.7B/models--Qwen--Qwen1.5-MoE-A2.7B/snapshots/1a758c50ecb6350748b9ce0a99d2352fd9fc11c9"
# ORIGINAL_MODEL_PATH = "/root/fsas/zhanghongyu/LAMoE/models/Qwen/expert_svd_router_redierct_k45"
ORIGINAL_MODEL_PATH = "/root/fsas/zhanghongyu/LAMoE/models/Qwen/expert_svd_router_redierct_k30"


# =========================================================
# 3. ä¸»å‡½æ•°
# =========================================================
if __name__ == "__main__":
    print(f"Loading model: {MODEL_FULL_NAME}")
    
    # åŠ è½½æ¨¡åž‹å’Œåˆ†è¯å™¨
    # tokenizer = AutoTokenizer.from_pretrained(CURRENT_MODEL_PATH)
    # model = AutoModelForCausalLM.from_pretrained(
    #     CURRENT_MODEL_PATH,
    #     torch_dtype=torch.bfloat16, # æ ¹æ®éœ€è¦è°ƒæ•´
    #     device_map="auto",
    #     trust_remote_code=True,
    # )

    # model = cast(Qwen2MoeForCausalLM, AutoModelForCausalLM.from_pretrained(
    #     BASE_MODEL_NAME,
    #     cache_dir = BASE_MODEL_PATH,
    #     dtype=torch.bfloat16,
    #     device_map="auto", 
    #     trust_remote_code=True,
    #     local_files_only=True
    # ))

    # tokenizer = AutoTokenizer.from_pretrained(
    #     BASE_MODEL_NAME, 
    #     cache_dir = BASE_MODEL_PATH,
    #     trust_remote_code=True,
    # )

    tokenizer = AutoTokenizer.from_pretrained(ORIGINAL_MODEL_PATH, trust_remote_code=True, local_files_only=True)
    model = cast(Qwen2MoeForCausalLM, AutoModelForCausalLM.from_pretrained(
        ORIGINAL_MODEL_PATH,
        dtype=torch.bfloat16, # ä¿æŒå’ŒåŽŸæ¨¡åž‹ä¸€è‡´
        device_map="auto",     # åœ¨CPUä¸ŠåŠ è½½ä»¥ä¿®æ”¹æƒé‡ï¼Œé¿å…GPUå†…å­˜é—®é¢˜
        trust_remote_code=True,
        local_files_only=True,
    ))

    # æ‰§è¡Œè§£ç å’Œæ—¥å¿—è®°å½•
    expert_selections = decode_and_log_expert_activations(model, tokenizer, NUM_DECODE_TOKENS)

    # === ç»“æžœåˆ†æžä¸Žä¿å­˜ ===
    print("\n--- Analysis of Expert Activations ---")
    total_activated_experts_per_layer = {}
    for layer_idx, selections in expert_selections.items():
        # å°†æ‰€æœ‰ token çš„ä¸“å®¶é€‰æ‹©åŽ‹å¹³åˆ°ä¸€ä¸ªé›†åˆä¸­ï¼Œä»¥è®¡ç®—å”¯ä¸€æ¿€æ´»çš„ä¸“å®¶æ•°é‡
        activated_experts = set(expert_id for token_experts in selections for expert_id in token_experts)
        total_activated_experts_per_layer[layer_idx] = sorted(list(activated_experts))
        
        print(f"Layer {layer_idx}:")
        print(f"  - Total unique experts activated: {len(activated_experts)}")
        # print(f"  - Activated expert IDs: {sorted(list(activated_experts))}")


    # === ä¿å­˜ä¸º JSON æ–‡ä»¶ ===
    output_dir = os.path.join(EVALUATE_DIR, "generation_expert_logs")
    os.makedirs(output_dir, exist_ok=True)

    # 1. ä¿å­˜è¯¦ç»†çš„é€ token è®°å½•
    detailed_log_path = os.path.join(output_dir, "expert_selections_per_token.json")
    with open(detailed_log_path, "w") as f:
        json.dump(expert_selections, f, indent=2)
    print(f"\n[Saved] Detailed per-token expert selections -> {detailed_log_path}")

    # 2. ä¿å­˜å”¯ä¸€æ¿€æ´»ä¸“å®¶çš„æ€»ç»“
    summary_log_path = os.path.join(output_dir, "unique_activated_experts_summary.json")
    with open(summary_log_path, "w") as f:
        # ä¸ºäº†å¯è¯»æ€§ï¼Œå°† value ä¹Ÿè½¬ä¸º str
        summary_data = {
            "total_unique_experts_per_layer": {k: len(v) for k, v in total_activated_experts_per_layer.items()},
            "activated_expert_ids_per_layer": total_activated_experts_per_layer,
        }
        json.dump(summary_data, f, indent=2, sort_keys=True)
    print(f"[Saved] Summary of unique activated experts -> {summary_log_path}")