import torch
import numpy as np
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.utils.data import DataLoader
from tqdm import tqdm
import torch.nn.functional as F
import json
import os
import sys
from transformers.models.qwen2_moe.modeling_qwen2_moe import Qwen2MoeSparseMoeBlock, Qwen2MoeMLP, Qwen2MoeForCausalLM, Qwen2MoeDecoderLayer
from typing import cast, List, Dict, Tuple

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
sys.path.append(project_root)

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

from src import config

# --- 1. é…ç½® ---
MODEL_NAME = "Qwen/expert_svd_router_avg_k45"
MODEL_PATH = "/root/fsas/zhanghongyu/LAMoE/models/Qwen/expert_svd_router_avg_k45"
OUTPUT_DIR = os.path.join(config.EVALUATE_DIR, "calibration_results")

# éªŒè¯çš„é…ç½®
VALIDATION_DATASET = "ai2_arc"
VALIDATION_SUBSET = "ARC-Challenge"
NUM_VALIDATION_SAMPLES = 1024
BATCH_SIZE = 1 # æ ¹æ®æ‚¨çš„ GPU æ˜¾å­˜è°ƒæ•´
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# æœç´¢èŒƒå›´ (æ¸©åº¦)
# T < 1.0 ä¼šä½¿åˆ†å¸ƒæ›´ "å°–é”" (sharper)
# T > 1.0 ä¼šä½¿åˆ†å¸ƒæ›´ "å¹³å¦" (flatter)
TEMP_RANGE = np.arange(0.21, 2.21, 0.02).tolist() 

# --- 2. è¾…åŠ©å‡½æ•°å’Œç±» ---
class RouterTemperatureHook:
    """
    ä¸€ä¸ª PyTorch Hook ç±»ï¼Œç”¨äºåœ¨ router çš„ forward pass åé€šè¿‡æ¸©åº¦ç¼©æ”¾ logitsã€‚
    """
    def __init__(self):
        self.temperature = 1.0

    def set_temperature(self, temperature: float):
        # é¿å…é™¤ä»¥é›¶
        self.temperature = max(temperature, 1e-9)

    def __call__(self, module, input, output):
        # output æ˜¯ router çš„åŸå§‹ logits
        original_logits = output
        
        # åº”ç”¨æ¸©åº¦ç¼©æ”¾
        adjusted_logits = original_logits / self.temperature
        
        return adjusted_logits

def prepare_arc_for_tcll(dataset_name, subset, num_samples, tokenizer):
    """
    åŠ è½½å¹¶é¢„å¤„ç† ARC æ•°æ®é›†ï¼Œä¸º TCLL è®¡ç®—åšå‡†å¤‡ã€‚
    """
    dataset = load_dataset(
        path=dataset_name,
        name=subset,
        split="train",
    ).select(range(num_samples))

    processed_samples = []
    for item in dataset:
        question = item['question']
        choices = item['choices']
        answer_key = item['answerKey']

        prompt = f"Question: {question}\nChoices:\n"
        for i, (label, text) in enumerate(zip(choices['label'], choices['text'])):
            prompt += f"{label}. {text}\n"

        prompt += "Answer:"
        
        correct_choice_label = answer_key
        # åŠ ä¸€ä¸ªç©ºæ ¼å‰ç¼€ ' A' ä»¥åŒ¹é…æ¨¡å‹ç”Ÿæˆä¹ æƒ¯
        target_token_id = tokenizer.encode(f" {correct_choice_label}")[0]

        input_ids = tokenizer.encode(prompt, return_tensors="pt")

        processed_samples.append({
            "input_ids": input_ids,
            "target_token_id": target_token_id
        })
        
    return processed_samples

# --- 3. ä¸»é€»è¾‘ ---

def main():
    print("ğŸš€ å¼€å§‹ Router Logits æ¸©åº¦æœç´¢æµç¨‹...")
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # --- æ­¥éª¤ 1: åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ---
    print(f"åŠ è½½æ¨¡å‹: {MODEL_PATH}")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, local_files_only=True)
    model = cast(Qwen2MoeForCausalLM, AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        local_files_only=True,
    ))
    model.eval()
    
    # --- æ­¥éª¤ 2: ä¸ºæ¯ä¸ªç›®æ ‡å±‚æ³¨å†Œ Hook ---
    hooks = []
    handles = []
    print("\nğŸ”§ ä¸ºæ¯ä¸ªç›®æ ‡ MoE å±‚æ³¨å†Œæ¸©åº¦ç¼©æ”¾ Hook...")
    layers_to_process = config.TARGET_LAYERS
    if not layers_to_process:
        print("âŒ é”™è¯¯: config.TARGET_LAYERS ä¸ºç©ºï¼Œè¯·æŒ‡å®šè¦åˆ†æçš„å±‚ã€‚")
        return

    for i, layer in enumerate(model.model.layers):
        if i in layers_to_process:
            try:
                router_module = cast(Qwen2MoeDecoderLayer, layer).mlp.gate
                hook = RouterTemperatureHook()
                handle = router_module.register_forward_hook(hook)
                
                hooks.append(hook)
                handles.append(handle)
                print(f"  - å·²åœ¨ç¬¬ {i} å±‚æ³¨å†Œ Hookã€‚")
            except AttributeError:
                print(f"âš ï¸ è­¦å‘Š: æ— æ³•åœ¨ç¬¬ {i} å±‚æ‰¾åˆ° 'mlp.gate'ï¼Œè·³è¿‡è¯¥å±‚ã€‚")
    
    if not handles:
        print("âŒ é”™è¯¯: æœªèƒ½æˆåŠŸæ³¨å†Œä»»ä½• Hookã€‚")
        return

    # --- æ­¥éª¤ 3: å‡†å¤‡éªŒè¯é›† ---
    print("\nğŸ“š å‡†å¤‡ ARC éªŒè¯é›†ç”¨äº TCLL è¯„ä¼°...")
    validation_data = prepare_arc_for_tcll(
        VALIDATION_DATASET, VALIDATION_SUBSET, NUM_VALIDATION_SAMPLES, tokenizer
    )
    
    # --- æ­¥éª¤ 4: Grid Search æ¸©åº¦å¹¶è¯„ä¼° TCLL ---
    print(f"\nğŸ” å¼€å§‹åœ¨ ARC éªŒè¯é›†ä¸Šæœç´¢æœ€ä½³æ¸©åº¦ï¼ŒèŒƒå›´: {TEMP_RANGE}")
    results = []

    for temp in TEMP_RANGE:
        for hook in hooks:
            hook.set_temperature(temp)
        
        total_tcll = 0.0
        
        with torch.no_grad():
            for sample in tqdm(validation_data, desc=f"è¯„ä¼° Temp={temp:.2f}"):
                input_ids = sample["input_ids"].to(DEVICE)
                target_token_id = sample["target_token_id"]

                outputs = model(input_ids)
                last_token_logits = outputs.logits[0, -1, :]

                log_probs = F.log_softmax(last_token_logits, dim=-1)
                tcll = log_probs[target_token_id].item()
                total_tcll += tcll

        avg_tcll = total_tcll / len(validation_data)
        
        print(f"Temp: {temp:.2f} -> å¹³å‡TCLL: {avg_tcll:.4f}")
        results.append({"temperature": temp, "avg_tcll": avg_tcll})

    # --- æ­¥éª¤ 5: é€‰å®šæœ€ä½³æ¸©åº¦å¹¶ä¿å­˜ç»“æœ ---
    for handle in handles:
        handle.remove()
    print("\nâœ… æ‰€æœ‰ Hook å·²è¢«ç§»é™¤ã€‚")

    # TCLL çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–ï¼Œæ‰€ä»¥æˆ‘ä»¬æ‰¾ avg_tcll æœ€å¤§çš„ç»“æœ
    best_result = max(results, key=lambda x: x["avg_tcll"])
    print("\nğŸ‰ æ¸©åº¦æœç´¢å®Œæˆï¼")
    print(f"æœ€ä½³æ¸©åº¦ (Temperature): {best_result['temperature']:.2f}")
    print(f"æœ€é«˜å¹³å‡ TCLL: {best_result['avg_tcll']:.4f}")

    output_file = os.path.join(OUTPUT_DIR, "calibration_results_temp.json")
    with open(output_file, "w") as f:
        json.dump(results, f, indent=2)
    print(f"è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {output_file}")
    
    print("\nä¸‹ä¸€æ­¥å»ºè®®:")
    print(f"ä½¿ç”¨é€‰å®šçš„ temperature = {best_result['temperature']:.2f} å‚æ•°ï¼Œåœ¨ä½ çš„è¯„ä¼°è„šæœ¬ä¸­è¿›è¡Œä¸€æ¬¡å®Œæ•´çš„è¯„ä¼°ã€‚")


if __name__ == "__main__":
    main()