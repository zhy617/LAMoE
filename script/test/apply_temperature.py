import torch
import os
import sys
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.models.qwen2_moe.modeling_qwen2_moe import Qwen2MoeForCausalLM, Qwen2MoeDecoderLayer
from typing import cast

# --- 1. é…ç½® ---

# è¾“å…¥ï¼šä½ åŸå§‹çš„ã€æœªç»ä¿®æ”¹çš„æ¨¡å‹è·¯å¾„
ORIGINAL_MODEL_PATH = "/root/fsas/zhanghongyu/LAMoE/models/Qwen/expert_svd_router_avg_k45"

# è¾“å‡ºï¼šä¿å­˜æ ¡å‡†åæ¨¡å‹çš„æ–°è·¯å¾„
CALIBRATED_MODEL_PATH = "/root/fsas/zhanghongyu/LAMoE/models/Qwen/expert_svd_router_avg_k45_temp_calibrated"

# å‚æ•°ï¼šä»ä½ çš„ search_temp_arc.py è„šæœ¬ä¸­æ‰¾åˆ°çš„æœ€ä½³æ¸©åº¦
# å‡è®¾ä½ æ‰¾åˆ°çš„æœ€ä½³æ¸©åº¦æ˜¯ 1.45ï¼Œè¯·åœ¨è¿™é‡Œä¿®æ”¹
BEST_TEMPERATURE = 1.35 

# --- 2. ä¸»é€»è¾‘ ---

def main():
    print(f"ğŸš€ å¼€å§‹åº”ç”¨æ¸©åº¦æ ¡å‡†...")
    print(f"æºæ¨¡å‹: {ORIGINAL_MODEL_PATH}")
    print(f"ç›®æ ‡æ¸©åº¦: {BEST_TEMPERATURE}")
    print(f"è¾“å‡ºè·¯å¾„: {CALIBRATED_MODEL_PATH}")

    if not (BEST_TEMPERATURE > 0):
        print("âŒ é”™è¯¯: æ¸©åº¦å¿…é¡»æ˜¯æ­£æ•°ã€‚")
        return

    # --- åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ---
    print("\nåŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(ORIGINAL_MODEL_PATH, trust_remote_code=True, local_files_only=True)
    model = cast(Qwen2MoeForCausalLM, AutoModelForCausalLM.from_pretrained(
        ORIGINAL_MODEL_PATH,
        dtype=torch.bfloat16, # ä¿æŒå’ŒåŸæ¨¡å‹ä¸€è‡´
        device_map="cpu",     # åœ¨CPUä¸ŠåŠ è½½ä»¥ä¿®æ”¹æƒé‡ï¼Œé¿å…GPUå†…å­˜é—®é¢˜
        trust_remote_code=True,
        local_files_only=True,
    ))
    model.eval()

    # --- åº”ç”¨æ¸©åº¦ç¼©æ”¾ ---
    print(f"\næ­£åœ¨å°†æ¸©åº¦ T={BEST_TEMPERATURE} åº”ç”¨åˆ° MoE å±‚çš„ router...")
    
    # åŠ¨æ€è·å–é¡¹ç›®æ ¹ç›®å½•å¹¶æ·»åŠ åˆ° sys.path
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
    sys.path.append(project_root)
    from src import config as AppConfig

    num_layers_changed = 0
    for i, layer in enumerate(model.model.layers):
        if i in AppConfig.TARGET_LAYERS:
            try:
                router_module = cast(Qwen2MoeDecoderLayer, layer).mlp.gate
                
                # ç›´æ¥ä¿®æ”¹ gate å±‚çš„æƒé‡å’Œåç½®
                # logits' = logits / T  ç­‰ä»·äº W' = W / T, b' = b / T
                with torch.no_grad():
                    router_module.weight.data /= BEST_TEMPERATURE
                    if router_module.bias is not None:
                        router_module.bias.data /= BEST_TEMPERATURE
                
                print(f"  - å·²æ ¡å‡†ç¬¬ {i} å±‚ routerã€‚")
                num_layers_changed += 1
            except AttributeError:
                print(f"âš ï¸ è­¦å‘Š: æ— æ³•åœ¨ç¬¬ {i} å±‚æ‰¾åˆ° 'mlp.gate'ï¼Œè·³è¿‡è¯¥å±‚ã€‚")

    if num_layers_changed == 0:
        print("âŒ é”™è¯¯: æ²¡æœ‰å¯¹ä»»ä½•å±‚è¿›è¡Œä¿®æ”¹ã€‚è¯·æ£€æŸ¥ `config.TARGET_LAYERS` é…ç½®ã€‚")
        return

    print(f"\nâœ… æˆåŠŸæ ¡å‡†äº† {num_layers_changed} ä¸ª MoE å±‚ã€‚")

    # --- ä¿å­˜ä¿®æ”¹åçš„æ¨¡å‹å’Œåˆ†è¯å™¨ ---
    print(f"\næ­£åœ¨ä¿å­˜æ ¡å‡†åçš„æ¨¡å‹åˆ°: {CALIBRATED_MODEL_PATH}")
    os.makedirs(CALIBRATED_MODEL_PATH, exist_ok=True)
    
    model.save_pretrained(CALIBRATED_MODEL_PATH)
    tokenizer.save_pretrained(CALIBRATED_MODEL_PATH)

    print("\nğŸ‰ æ ¡å‡†ç‰ˆæ¨¡å‹ä¿å­˜å®Œæ¯•ï¼")
    print("\nä¸‹ä¸€æ­¥:")
    print("åœ¨ä½ çš„ lm-evaluation-harness å‘½ä»¤ä¸­ï¼Œä½¿ç”¨ä¸‹é¢çš„è·¯å¾„ä½œä¸º model_nameï¼š")
    print(f"  --model_name {CALIBRATED_MODEL_PATH}")


if __name__ == "__main__":
    main()